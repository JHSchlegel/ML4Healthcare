{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for NAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will tune the most important hyperparameters of the Neural Additive Model (NAM) using Optuna since we realised during training that the model is very sensitive to the choice of some hyperparameters. We will propose similar hyperparameter values as the ones used in the [NAM paper](https://arxiv.org/pdf/2004.13912.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimpy import clean_columns\n",
    "from pickle import load\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from torch.utils.data import DataLoader\n",
    "from neural_additive_model import NAM\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "import logging\n",
    "\n",
    "\n",
    "# append path to parent folder to allow imports from utils folder\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "from utils.utils import (\n",
    "    set_all_seeds,\n",
    "    HeartFailureDataset,\n",
    "    train_and_validate_one_epoch,\n",
    "    get_n_units,\n",
    "    penalized_binary_cross_entropy,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "N_TRIALS = 1_000\n",
    "N_EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/heart_failure/train_val_split.csv\").pipe(\n",
    "    clean_columns\n",
    ")\n",
    "X = train_df.drop(columns=[\"heart_disease\"], axis=1)\n",
    "outlier_idx = X.query(\"resting_bp == 0\").index\n",
    "X = X.drop(outlier_idx)\n",
    "y = train_df[\"heart_disease\"]\n",
    "y = y[X.index]\n",
    "\n",
    "# create categorical variable for cholesterol level\n",
    "X[\"chol_level\"] = pd.cut(\n",
    "    X[\"cholesterol\"],\n",
    "    bins=[-1, 10, 200, 240, 1000],\n",
    "    labels=[\"imputed\", \"normal\", \"borderline\", \"high\"],\n",
    ")\n",
    "\n",
    "X.index = range(len(X))\n",
    "y.index = range(len(y))\n",
    "\n",
    "\n",
    "# load and apply preprocessor:\n",
    "preprocessor = load(open(\"../models/preprocessor.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objective function for optuna\n",
    "def objective(trial):\n",
    "    NAM_params = {\n",
    "        \"out_size\": 1,\n",
    "        \"hidden_profile\": trial.suggest_categorical(\n",
    "            \"hidden_profile\", [\n",
    "                [64, 64, 32],\n",
    "                [1024],\n",
    "                [256],\n",
    "                [128, 256, 128, 64],\n",
    "                [256, 128, 128],\n",
    "            ]\n",
    "        ),\n",
    "        \"use_exu\": False, \n",
    "        \"use_relu_n\": False, \n",
    "        \"within_feature_dropout\": trial.suggest_categorical(\n",
    "            \"within_feature_dropout\",\n",
    "            [0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        ),\n",
    "        \"feature_dropout\": trial.suggest_categorical(\n",
    "            \"feature_dropout\", [0, 0.05, 0.1, 0.2]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    pen_bce_params = {\n",
    "        \"output_regularization\": trial.suggest_float(\"output_regularization\", 1e-3, 1e-1, log=True),\n",
    "        \"l2_regularization\": trial.suggest_float(\"l2_regularization\", 1e-6, 1e-4, log=True),\n",
    "    }\n",
    "        \n",
    "    adam_params = {\"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)}\n",
    "   \n",
    "    set_all_seeds(SEED)\n",
    "    \n",
    "    balanced_accuracies = {}\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n",
    "    for fold_num, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx].to_numpy(), y[val_idx].to_numpy()\n",
    "        \n",
    "        X_train = preprocessor.fit_transform(X_train)\n",
    "        X_val = preprocessor.transform(X_val)\n",
    "        \n",
    "        NAM_params[\"n_features\"] = X_train.shape[1]\n",
    "        NAM_params[\"in_size\"] = get_n_units(X_train)\n",
    "        \n",
    "        set_all_seeds(SEED)\n",
    "        model = NAM(**NAM_params).to(DEVICE)\n",
    "        \n",
    "         \n",
    "        optimizer = torch.optim.Adam(model.parameters(), **adam_params)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "        \n",
    "        # Create the dataset and dataloader\n",
    "        train_dataset = HeartFailureDataset(X_train, y_train)\n",
    "        val_dataset = HeartFailureDataset(X_val, y_val)\n",
    "\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=32, shuffle=True, pin_memory=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, pin_memory=True)\n",
    "        \n",
    "        balanced_accuracies[fold_num] = []\n",
    "        #f1_scores[fold_num] = []\n",
    "        \n",
    "        for epoch in range(N_EPOCHS):\n",
    "            set_all_seeds(SEED)\n",
    "            _,_, _, balanced_accuracy = train_and_validate_one_epoch(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                criterion=penalized_binary_cross_entropy,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                device=DEVICE,\n",
    "                scheduler=scheduler,\n",
    "                use_penalized_BCE=True,\n",
    "                **pen_bce_params,\n",
    "            )\n",
    "            \n",
    "            balanced_accuracies[fold_num].append(balanced_accuracy)\n",
    "            #f1_scores[fold_num].append(f1_score)\n",
    "            \n",
    "            trial.report(balanced_accuracy, epoch) #(balanced_accuracy + f1_score) / 2, fold_num + epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "            \n",
    "        \n",
    "        return (np.mean([lst[-5:] for lst in balanced_accuracies.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logger and log optuna trials to log file\n",
    "logging.basicConfig(filename=\"hyperparameter_tuning.log\", level=logging.INFO)\n",
    "\n",
    "optuna.logging.enable_propagation()\n",
    "optuna.logging.disable_default_handler()\n",
    "\n",
    "# prune bad trials since NAM is computationally expensive\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=20)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"NAM_hyperparameter_tuning\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    pruner=pruner,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a27d8ec4af4c17bce6701315535192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-04-08 22:45:23,053] Trial 0 finished with value: 0.8421444527177959 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0.6, 'feature_dropout': 0.1, 'output_regularization': 0.0038234752246751854, 'l2_regularization': 1.6738085788752145e-05, 'lr': 0.00026210878782654407}. Best is trial 0 with value: 0.8421444527177959.\n",
      "[I 2024-04-08 22:47:26,265] Trial 1 finished with value: 0.6235107967237528 and parameters: {'hidden_profile': [128, 256, 128, 64, 32], 'within_feature_dropout': 0.7, 'feature_dropout': 0.05, 'output_regularization': 0.009780337016659405, 'l2_regularization': 1.1715937392307063e-06, 'lr': 0.053451661106468214}. Best is trial 0 with value: 0.8421444527177959.\n",
      "[I 2024-04-08 22:48:13,898] Trial 2 finished with value: 0.8842144452717795 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0.1, 'feature_dropout': 0.2, 'output_regularization': 0.005170191786366992, 'l2_regularization': 3.646439558980721e-06, 'lr': 0.004247058562261873}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 22:50:15,660] Trial 3 finished with value: 0.5 and parameters: {'hidden_profile': [128, 256, 128, 64, 32], 'within_feature_dropout': 0.9, 'feature_dropout': 0, 'output_regularization': 0.004470608546778492, 'l2_regularization': 2.878805718308926e-05, 'lr': 0.008178476574339543}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 22:51:34,342] Trial 4 finished with value: 0.8642032762472078 and parameters: {'hidden_profile': [64, 64, 32], 'within_feature_dropout': 0.1, 'feature_dropout': 0.05, 'output_regularization': 0.03244160088734161, 'l2_regularization': 2.868113482103008e-06, 'lr': 0.00017019223026554014}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 22:51:41,469] Trial 5 pruned. \n",
      "[I 2024-04-08 22:53:27,777] Trial 6 finished with value: 0.8474497393894268 and parameters: {'hidden_profile': [128, 256, 256, 128], 'within_feature_dropout': 0.2, 'feature_dropout': 0.2, 'output_regularization': 0.003013864904679803, 'l2_regularization': 1.9489008462344274e-06, 'lr': 0.002940074130903307}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 22:53:33,215] Trial 7 pruned. \n",
      "[I 2024-04-08 22:55:12,373] Trial 8 finished with value: 0.8441921072226359 and parameters: {'hidden_profile': [128, 256, 256, 128], 'within_feature_dropout': 0.1, 'feature_dropout': 0.05, 'output_regularization': 0.004993662066506421, 'l2_regularization': 2.830814149699807e-05, 'lr': 0.04912819179585589}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 22:56:32,059] Trial 9 finished with value: 0.8642032762472078 and parameters: {'hidden_profile': [64, 64, 32], 'within_feature_dropout': 0.05, 'feature_dropout': 0.05, 'output_regularization': 0.031115931750282644, 'l2_regularization': 1.991892049219752e-05, 'lr': 0.03529150925129901}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 22:57:10,760] Trial 10 finished with value: 0.8705323901712584 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0, 'feature_dropout': 0.2, 'output_regularization': 0.0010862348973937122, 'l2_regularization': 5.5637508572292e-06, 'lr': 0.0018842882788808852}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 22:57:50,501] Trial 11 finished with value: 0.8705323901712584 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0, 'feature_dropout': 0.2, 'output_regularization': 0.0010233491820915094, 'l2_regularization': 8.611382274356348e-05, 'lr': 0.0020885631616124442}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 22:58:37,791] Trial 12 finished with value: 0.8631794489947877 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0.4, 'feature_dropout': 0.2, 'output_regularization': 0.0010705097097397317, 'l2_regularization': 6.181540804056209e-06, 'lr': 0.0010558817709141067}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 22:59:19,897] Trial 13 finished with value: 0.8821667907669397 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0, 'feature_dropout': 0.2, 'output_regularization': 0.09613038630093086, 'l2_regularization': 4.060545468633147e-06, 'lr': 0.00994431451255696}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 23:00:06,865] Trial 14 finished with value: 0.8621556217423677 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0.3, 'feature_dropout': 0.2, 'output_regularization': 0.09818226878189122, 'l2_regularization': 3.0743260265507575e-06, 'lr': 0.010040072096533233}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 23:00:54,118] Trial 15 finished with value: 0.858060312732688 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0.5, 'feature_dropout': 0, 'output_regularization': 0.08106561872514703, 'l2_regularization': 3.2551033002761964e-06, 'lr': 0.009351736864022756}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 23:01:41,040] Trial 16 finished with value: 0.8494973938942666 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0.1, 'feature_dropout': 0.2, 'output_regularization': 0.020196606010770102, 'l2_regularization': 1.4793050252991514e-06, 'lr': 0.01798209445013631}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 23:01:44,527] Trial 17 pruned. \n",
      "[I 2024-04-08 23:03:31,833] Trial 18 finished with value: 0.875837676842889 and parameters: {'hidden_profile': [128, 256, 128, 64, 32], 'within_feature_dropout': 0, 'feature_dropout': 0.2, 'output_regularization': 0.05319301949684396, 'l2_regularization': 4.0972127413567e-06, 'lr': 0.01788936491032311}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 23:04:18,514] Trial 19 finished with value: 0.8631794489947877 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0.4, 'feature_dropout': 0.1, 'output_regularization': 0.01786470502316383, 'l2_regularization': 9.189941362445046e-06, 'lr': 0.0011527311323539671}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 23:05:03,727] Trial 20 finished with value: 0.8831906180193595 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0.2, 'feature_dropout': 0, 'output_regularization': 0.007502725484689863, 'l2_regularization': 2.1345401933097325e-06, 'lr': 0.004122935449887561}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 23:05:51,575] Trial 21 finished with value: 0.876861504095309 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0.2, 'feature_dropout': 0, 'output_regularization': 0.006715468151400539, 'l2_regularization': 1.9677028973131003e-06, 'lr': 0.005332922211394267}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 23:06:37,228] Trial 22 finished with value: 0.8568503350707373 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0.2, 'feature_dropout': 0, 'output_regularization': 0.002271016670178202, 'l2_regularization': 2.3045254190652792e-06, 'lr': 0.022364520040551658}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 23:07:19,534] Trial 23 finished with value: 0.876861504095309 and parameters: {'hidden_profile': [1024], 'within_feature_dropout': 0.05, 'feature_dropout': 0, 'output_regularization': 0.015494519544472615, 'l2_regularization': 4.452334721417402e-06, 'lr': 0.0045091859685443755}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[I 2024-04-08 23:07:22,236] Trial 24 pruned. \n",
      "[I 2024-04-08 23:07:24,996] Trial 25 pruned. \n",
      "[I 2024-04-08 23:07:33,326] Trial 26 pruned. \n",
      "[I 2024-04-08 23:08:51,980] Trial 27 finished with value: 0.8568503350707373 and parameters: {'hidden_profile': [64, 64, 32], 'within_feature_dropout': 0.3, 'feature_dropout': 0, 'output_regularization': 0.013668429529370467, 'l2_regularization': 1.6268296613238986e-06, 'lr': 0.006097600512353361}. Best is trial 2 with value: 0.8842144452717795.\n",
      "[W 2024-04-08 23:10:01,255] Trial 28 failed with parameters: {'hidden_profile': [128, 256, 256, 128], 'within_feature_dropout': 0, 'feature_dropout': 0.1, 'output_regularization': 0.049183041824066215, 'l2_regularization': 2.666453787443785e-06, 'lr': 0.0015909781087048952} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/janhsc/miniconda3/envs/ml4hc/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_1517382/4024250325.py\", line 69, in objective\n",
      "    _,_, _, balanced_accuracy = train_and_validate_one_epoch(\n",
      "  File \"/home/janhsc/Documents/ETH/Master 2. Semester/ML4HC/ML4Healthcare/project1/task1/neural_additive_model/../../utils/utils.py\", line 232, in train_and_validate_one_epoch\n",
      "    loss = penalized_binary_cross_entropy(\n",
      "  File \"/home/janhsc/Documents/ETH/Master 2. Semester/ML4HC/ML4Healthcare/project1/task1/neural_additive_model/../../utils/utils.py\", line 693, in penalized_binary_cross_entropy\n",
      "    weight_decay_feature_params(model, num_networks) * l2_regularization\n",
      "  File \"/home/janhsc/Documents/ETH/Master 2. Semester/ML4HC/ML4Healthcare/project1/task1/neural_additive_model/../../utils/utils.py\", line 650, in weight_decay_feature_params\n",
      "    feature_weights_losses = [\n",
      "  File \"/home/janhsc/Documents/ETH/Master 2. Semester/ML4HC/ML4Healthcare/project1/task1/neural_additive_model/../../utils/utils.py\", line 654, in <listcomp>\n",
      "    if (param.requires_grad == True) and (\"weights\" in name)\n",
      "KeyboardInterrupt\n",
      "[W 2024-04-08 23:10:01,256] Trial 28 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# timeout after 20 hours\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4hc/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4hc/lib/python3.10/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4hc/lib/python3.10/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4hc/lib/python3.10/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4hc/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[14], line 69\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[1;32m     68\u001b[0m     set_all_seeds(SEED)\n\u001b[0;32m---> 69\u001b[0m     _,_, _, balanced_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalized_binary_cross_entropy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_penalized_BCE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpen_bce_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     balanced_accuracies[fold_num]\u001b[38;5;241m.\u001b[39mappend(balanced_accuracy)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m#f1_scores[fold_num].append(f1_score)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ETH/Master 2. Semester/ML4HC/ML4Healthcare/project1/task1/neural_additive_model/../../utils/utils.py:232\u001b[0m, in \u001b[0;36mtrain_and_validate_one_epoch\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, use_penalized_BCE, output_regularization, l2_regularization, scheduler, device)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_penalized_BCE:\n\u001b[1;32m    231\u001b[0m     aggregated_logits, feature_logits \u001b[38;5;241m=\u001b[39m model(x\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m--> 232\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mpenalized_binary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregated_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_regularization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_regularization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml2_regularization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_regularization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(x\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/Documents/ETH/Master 2. Semester/ML4HC/ML4Healthcare/project1/task1/neural_additive_model/../../utils/utils.py:693\u001b[0m, in \u001b[0;36mpenalized_binary_cross_entropy\u001b[0;34m(model, aggregated_logits, feature_logits, y_true, output_regularization, l2_regularization)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l2_regularization \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    691\u001b[0m     num_networks \u001b[38;5;241m=\u001b[39m feature_logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    692\u001b[0m     regularization_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 693\u001b[0m         \u001b[43mweight_decay_feature_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_networks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m l2_regularization\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss \u001b[38;5;241m+\u001b[39m regularization_loss\n",
      "File \u001b[0;32m~/Documents/ETH/Master 2. Semester/ML4HC/ML4Healthcare/project1/task1/neural_additive_model/../../utils/utils.py:650\u001b[0m, in \u001b[0;36mweight_decay_feature_params\u001b[0;34m(model, num_networks)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate penalty term for the weights of the feature networks.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Penalty term for the weights of the feature networks\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;66;03m# note that tf.nn.l2_loss divides the squared Euclidean/Frobenius norm by 2\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;66;03m# for more information: https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m feature_weights_losses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    651\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnorm(param, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters()\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;66;03m# only penalize weights that require gradients; don't penalize biases\u001b[39;00m\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name)\n\u001b[1;32m    655\u001b[0m ]\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mstack(feature_weights_losses)) \u001b[38;5;241m/\u001b[39m num_networks\n",
      "File \u001b[0;32m~/Documents/ETH/Master 2. Semester/ML4HC/ML4Healthcare/project1/task1/neural_additive_model/../../utils/utils.py:654\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate penalty term for the weights of the feature networks.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Penalty term for the weights of the feature networks\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;66;03m# note that tf.nn.l2_loss divides the squared Euclidean/Frobenius norm by 2\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;66;03m# for more information: https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss\u001b[39;00m\n\u001b[1;32m    650\u001b[0m feature_weights_losses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    651\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnorm(param, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters()\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;66;03m# only penalize weights that require gradients; don't penalize biases\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name)\n\u001b[1;32m    655\u001b[0m ]\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mstack(feature_weights_losses)) \u001b[38;5;241m/\u001b[39m num_networks\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study.optimize(\n",
    "    objective, \n",
    "    n_trials=N_TRIALS,\n",
    "    timeout = 20 * 60 * 60, # timeout after 20 hours\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_profile': [1024], 'use_exu': False, 'use_relu_n': True, 'within_feature_dropout': 0.2, 'feature_dropout': 0.2, 'output_regularization': 0.0044043429133187205, 'l2_regularization': 1.6634538799288264e-06, 'lr': 0.0016138573407084142}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "best_params = study.best_params\n",
    "\n",
    "config_path = \"config.yaml\"\n",
    "\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(best_params, file)\n",
    "    \n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "cliponaxis": false,
         "hovertemplate": [
          "output_regularization (FloatDistribution): 0.02284943022528039<extra></extra>",
          "feature_dropout (CategoricalDistribution): 0.023135498105448<extra></extra>",
          "use_exu (CategoricalDistribution): 0.047640549033518116<extra></extra>",
          "l2_regularization (FloatDistribution): 0.04850835536546978<extra></extra>",
          "within_feature_dropout (CategoricalDistribution): 0.10529642094118068<extra></extra>",
          "use_relu_n (CategoricalDistribution): 0.11564166138647725<extra></extra>",
          "lr (FloatDistribution): 0.20149830559605955<extra></extra>",
          "hidden_profile (CategoricalDistribution): 0.4354297793465662<extra></extra>"
         ],
         "name": "Objective Value",
         "orientation": "h",
         "text": [
          "0.02",
          "0.02",
          "0.05",
          "0.05",
          "0.11",
          "0.12",
          "0.20",
          "0.44"
         ],
         "textposition": "outside",
         "type": "bar",
         "x": [
          0.02284943022528039,
          0.023135498105448,
          0.047640549033518116,
          0.04850835536546978,
          0.10529642094118068,
          0.11564166138647725,
          0.20149830559605955,
          0.4354297793465662
         ],
         "y": [
          "output_regularization",
          "feature_dropout",
          "use_exu",
          "l2_regularization",
          "within_feature_dropout",
          "use_relu_n",
          "lr",
          "hidden_profile"
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Hyperparameter Importances"
        },
        "xaxis": {
         "title": {
          "text": "Hyperparameter Importance"
         }
        },
        "yaxis": {
         "title": {
          "text": "Hyperparameter"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_importance = optuna.visualization.plot_param_importances(study)\n",
    "p_importance.write_image(\"param_importance.png\")\n",
    "p_importance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          6,
          11,
          14,
          19,
          22,
          24,
          27,
          28,
          31,
          32,
          35,
          40,
          51,
          52,
          53,
          58,
          61,
          62,
          63,
          64,
          70,
          72,
          73,
          74,
          81,
          91,
          101,
          104,
          112,
          114,
          121,
          123,
          131,
          132,
          141,
          142,
          145,
          151,
          161,
          162,
          167,
          171,
          174,
          178,
          181,
          183,
          191,
          192,
          196,
          198,
          201,
          202,
          211,
          213,
          217,
          221,
          222,
          223,
          231,
          233,
          237,
          241,
          242,
          244,
          245,
          260,
          264,
          266,
          290,
          294,
          296,
          304,
          313,
          328,
          338,
          342,
          345,
          352,
          353,
          361,
          366,
          369,
          387,
          395,
          398,
          411,
          418,
          426,
          433,
          436,
          456,
          468,
          469,
          475,
          478,
          489,
          496,
          504,
          506,
          509,
          514,
          520,
          524,
          531,
          534,
          536,
          546,
          557,
          564,
          577,
          585,
          601,
          611,
          632,
          649,
          650,
          655,
          660,
          662,
          665,
          672,
          676,
          677,
          679,
          683,
          684,
          698,
          702,
          703,
          709,
          717,
          728,
          734,
          740,
          746,
          751,
          756,
          779,
          797,
          799,
          813,
          819,
          824,
          839,
          847,
          854,
          871,
          875,
          885,
          891,
          909,
          915,
          922,
          926,
          932,
          944,
          945,
          962,
          968,
          978,
          983,
          984,
          989,
          995,
          998
         ],
         "y": [
          0.8626623376623377,
          0.8547619047619047,
          0.8002247752247753,
          0.8958344846642718,
          0.6875859434682965,
          0.8894411926102066,
          0.8958344846642718,
          0.9033016279495152,
          0.908756890591423,
          0.8791242541242541,
          0.8969030969030969,
          0.9033016279495152,
          0.9033016279495152,
          0.9033016279495152,
          0.9033016279495152,
          0.9033016279495152,
          0.9022727272727273,
          0.9033016279495152,
          0.9033016279495152,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9033016279495152,
          0.9033016279495152,
          0.9097448650640139,
          0.9033016279495152,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9033016279495152,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9033016279495152,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9033016279495152,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9236552454637561,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9033016279495152,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139
         ]
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937,
          938,
          939,
          940,
          941,
          942,
          943,
          944,
          945,
          946,
          947,
          948,
          949,
          950,
          951,
          952,
          953,
          954,
          955,
          956,
          957,
          958,
          959,
          960,
          961,
          962,
          963,
          964,
          965,
          966,
          967,
          968,
          969,
          970,
          971,
          972,
          973,
          974,
          975,
          976,
          977,
          978,
          979,
          980,
          981,
          982,
          983,
          984,
          985,
          986,
          987,
          988,
          989,
          990,
          991,
          992,
          993,
          994,
          995,
          996,
          997,
          998,
          999
         ],
         "y": [
          0.8626623376623377,
          0.8626623376623377,
          0.8626623376623377,
          0.8958344846642718,
          0.8958344846642718,
          0.8958344846642718,
          0.8958344846642718,
          0.8958344846642718,
          0.8958344846642718,
          0.8958344846642718,
          0.8958344846642718,
          0.8958344846642718,
          0.8958344846642718,
          0.8958344846642718,
          0.9033016279495152,
          0.9033016279495152,
          0.9033016279495152,
          0.9033016279495152,
          0.9033016279495152,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.908756890591423,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9097448650640139,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561,
          0.9236552454637561
         ]
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "title": {
          "text": "Trial"
         }
        },
        "yaxis": {
         "title": {
          "text": "Objective Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_history = optuna.visualization.plot_optimization_history(study)\n",
    "p_history.write_image(\"optimization_history.png\")\n",
    "p_history.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4hc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
