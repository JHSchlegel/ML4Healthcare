{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PTBDataset, test\n\u001b[0;32m     29\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "import math\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "from utils import PTBDataset, test\n",
    "from torch import Tensor\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'C:\\Users\\pauls\\Desktop\\Studium\\Machine Learning for Health Care\\Projekt 2\\project2_TS_input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(path+'\\ptbdb_train.csv', header=None)\n",
    "test_df = pd.read_csv(path+'\\ptbdb_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = train_df.iloc[:, :-1].to_numpy()\n",
    "y_train_full = train_df.iloc[:, -1].to_numpy()\n",
    "\n",
    "X_test = test_df.iloc[:, :-1].to_numpy()\n",
    "y_test = test_df.iloc[:, -1].to_numpy()\n",
    "\n",
    "X_train_full = np.c_[X_train_full, np.zeros((X_train_full.shape[0], 3))]\n",
    "X_test = np.c_[X_test, np.zeros((X_test.shape[0], 3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got this function from IML\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_loader_from_np(X, y = None, train = True, batch_size=32, shuffle=True, num_workers = 4):\n",
    "    \"\"\"\n",
    "    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n",
    "\n",
    "    input: X: numpy array, the features\n",
    "           y: numpy array, the labels\n",
    "    \n",
    "    output: loader: torch.data.util.DataLoader, the object containing the data\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        # Attention: If you get type errors you can modify the type of the\n",
    "        # labels here\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), \n",
    "                                torch.from_numpy(y).type(torch.long))\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        pin_memory=True, num_workers=num_workers)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_loader_from_np(X_train, y_train, train = True, batch_size=32)\n",
    "val_loader = create_loader_from_np(X_val, y_val, train = True, shuffle = False, batch_size=32)\n",
    "test_loader = create_loader_from_np(X_test, y_test, train = False, shuffle = False, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies that I need because I can't import the utils file. Note: I made changes in train_one_epoch, validate_one_epoch and test_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script contains all the utility functions for the Transformer model.\n",
    "\n",
    "# =========================================================================== #\n",
    "#                              Packages and Presets                           #\n",
    "# =========================================================================== #\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.parameter import Parameter\n",
    "from tqdm import trange, tqdm\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, accuracy_score\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# =========================================================================== #\n",
    "#                 Data Loading and Preprocessing Utilities                    #\n",
    "# =========================================================================== #\n",
    "class PTB_Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(2)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        # Check if the input tensor is 3D, if so, add a batch dimension\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "# =========================================================================== #\n",
    "#                             General Utilities                               #                \n",
    "# =========================================================================== #\n",
    "# see https://vandurajan91.medium.com/random-seeds-and-reproducible-results-in-pytorch-211620301eba\n",
    "# for more information aboutreproducibility in pytorch\n",
    "def set_all_seeds(seed: int):\n",
    "    \"\"\"Set all possible seeds to ensure reproducibility and to avoid randomness\n",
    "    involved in GPU computations.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "#!!! This Early Stopping class is inspired by the following stackoverflow post:\n",
    "#!!! https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "#!!! and by this kaggle notebook:\n",
    "#!!! https://www.kaggle.com/code/megazotya/ecg-transformer/notebook\n",
    "class EarlyStopping:\n",
    "    def __init__(\n",
    "        self, \n",
    "        start: int = 50,\n",
    "        patience: int = 10,\n",
    "        epsilon: float = 1e-6,\n",
    "        verbose: bool = False,\n",
    "        mode: str = \"min\"\n",
    "    ):\n",
    "        self.start = start\n",
    "        self.counter = 0\n",
    "        self.patience = patience\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        self.mode = mode\n",
    "        \n",
    "        # initialize objects of which best value will be tracked\n",
    "        self.best_model = nn.Identity\n",
    "        self.best_epoch = 0\n",
    "        self.best_score = np.inf if mode == \"min\" else -np.inf\n",
    "        \n",
    "        \n",
    "    def early_stop(self, model: nn.Module, metric: float, current_epoch: int) -> bool:\n",
    "        \"\"\"Whether training should be stopped or not. If there was no improvement\n",
    "        in a long time, training should be stopped. Continuously saves the best\n",
    "        model.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current validation loss\n",
    "            model (nn.Module): Current model\n",
    "            current_epoch (int): Current epoch number\n",
    "\n",
    "        Returns:\n",
    "            bool: Whether training should be stopped or not\n",
    "        \"\"\"\n",
    "        # check whether improvement was large enough (if there was any at all)\n",
    "        if (\n",
    "            (metric < self.best_score + self.epsilon and self.mode == \"min\") or\n",
    "            (metric > self.best_score - self.epsilon and self.mode == \"max\")\n",
    "        ):\n",
    "            # reset number of epochs without improvement\n",
    "            self.counter = 0\n",
    "            # update best model and best score\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_score = metric\n",
    "            self.best_epoch = current_epoch\n",
    "            \n",
    "        elif current_epoch > self.start:\n",
    "            self.counter += 1  # stop training if no improvement in a long time\n",
    "            if self.counter >= self.patience:\n",
    "                if self.verbose:\n",
    "                    print(f\"Early stopping at epoch {current_epoch}. Best score was {self.best_score:.4f} in epoch {self.best_epoch}.\")\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def save_best_model(self, model_path:str) -> None:\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        torch.save(self.best_model, model_path)\n",
    "        \n",
    "    def get_best_model(self) -> nn.Module:\n",
    "        model = torch.load_state_dict(self.best_model)\n",
    "        return model\n",
    "\n",
    "\n",
    "# =========================================================================== #\n",
    "#                        Transformer Specific Utilities                       #\n",
    "# =========================================================================== #\n",
    "# Custom weight initialization:\n",
    "#!!! Copied from https://www.kaggle.com/code/megazotya/ecg-transformer/notebook\n",
    "def init_parameters(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "# =========================================================================== #\n",
    "#                    Model Training and Evaluation                            #\n",
    "# =========================================================================== #\n",
    "def train_one_epoch(\n",
    "    model:nn.Module,\n",
    "    optimizer:optim.Optimizer,\n",
    "    criterion:nn.Module,\n",
    "    train_loader:DataLoader,\n",
    "    device:torch.device\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"Train the model for one epoch\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to train\n",
    "        optimizer (optim.Optimizer): Optimizer to use\n",
    "        criterion (nn.Module): Loss function\n",
    "        train_loader (DataLoader): Train data loader\n",
    "        device (torch.device): Device on which calculations are executed\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float, float, float]: Tuple of train loss, accuracy, \n",
    "            balanced accuracy and f1 score\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    y_preds = []\n",
    "    y_true = []\n",
    "    for seq, label in train_loader:\n",
    "        seq, label = seq.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #! Note Paul: I added this in order for the model not to give me any dimension issues!\n",
    "        seq=seq.squeeze(1)\n",
    "        \n",
    "        output = model(seq)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        y_true.extend(label.cpu().numpy())\n",
    "        y_preds.extend(F.softmax(output, dim=1).argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    \n",
    "    # calculate metrics\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    train_acc = accuracy_score(y_true, y_preds)\n",
    "    train_balanced_acc = balanced_accuracy_score(y_true, y_preds)\n",
    "    train_f1_score = f1_score(y_true, y_preds)\n",
    "    return train_loss, train_acc, train_balanced_acc, train_f1_score\n",
    "\n",
    "def validate_one_epoch(\n",
    "    model:nn.Module,\n",
    "    criterion:nn.Module,\n",
    "    val_loader:DataLoader,\n",
    "    device:torch.device\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"Validate the model for one epoch\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to validate\n",
    "        criterion (nn.Module): Loss function to use\n",
    "        val_loader (DataLoader): Validation data loader\n",
    "        device (torch.device): Device on which calculations are executed\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float, float, float]: tuple of validation loss, accuracy,\n",
    "            balanced accuracy and f1 score\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    y_preds = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for seq, label in val_loader:\n",
    "            seq, label = seq.to(device), label.to(device)\n",
    "            \n",
    "            #! Note Paul: I added this in order for the model not to give me any dimension issues!\n",
    "            seq=seq.squeeze(1)\n",
    "            \n",
    "            output = model(seq)\n",
    "            \n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            y_true.extend(label.cpu().numpy())\n",
    "            y_preds.extend(F.softmax(output, dim=1).argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    \n",
    "    # calculate metrics\n",
    "    val_loss = total_loss / len(val_loader.dataset)\n",
    "    val_acc = accuracy_score(y_true, y_preds)\n",
    "    val_balanced_acc = balanced_accuracy_score(y_true, y_preds)\n",
    "    val_f1_score = f1_score(y_true, y_preds)\n",
    "    return val_loss, val_acc, val_balanced_acc, val_f1_score\n",
    "\n",
    "def train_and_validate(\n",
    "    model:nn.Module,\n",
    "    optimizer:optim.Optimizer,\n",
    "    scheduler:optim.lr_scheduler,\n",
    "    criterion:nn.Module,\n",
    "    train_loader:DataLoader,\n",
    "    val_loader:DataLoader,\n",
    "    best_model_path:str=\"weights/transformer_pe.pth\",\n",
    "    device:torch.device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    num_epochs:int=100,\n",
    "    ES:EarlyStopping=None,\n",
    "    summary_writer:SummaryWriter=None\n",
    "):\n",
    "    # make fancy progress bar\n",
    "    with trange(num_epochs) as t:\n",
    "        for epoch in t:\n",
    "            dct = {}\n",
    "            train_loss, train_acc, train_balanced_acc, train_f1_score = train_one_epoch(model, optimizer, criterion, train_loader, device)\n",
    "            val_loss, val_acc, val_balanced_acc, val_f1_score = validate_one_epoch(model, criterion, val_loader, device)\n",
    "            \n",
    "            # update progress bar\n",
    "            t.set_description(f\"Training Transformer\")\n",
    "            t.set_postfix(\n",
    "                train_loss=train_loss,\n",
    "                val_loss=val_loss,\n",
    "                train_balanced_acc=train_balanced_acc,\n",
    "                val_balanced_acc=val_balanced_acc\n",
    "            )\n",
    "            \n",
    "            \n",
    "            # reduce learning rate\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "            # log metrics to tensorboard\n",
    "            if summary_writer is not None:\n",
    "                summary_writer.add_scalar(\"Loss train\", train_loss, epoch)\n",
    "                summary_writer.add_scalar(\"Loss val\", val_loss, epoch)\n",
    "                summary_writer.add_scalar(\"Accuracy train\", train_acc, epoch)\n",
    "                summary_writer.add_scalar(\"Accuracy val\", val_acc, epoch)\n",
    "                summary_writer.add_scalar(\"Balanced accuracy train\", train_balanced_acc, epoch)\n",
    "                summary_writer.add_scalar(\"Balanced accuracy val\", val_balanced_acc, epoch)\n",
    "                summary_writer.add_scalar(\"F1 score train\", train_f1_score, epoch)\n",
    "                summary_writer.add_scalar(\"F1 score val\", val_f1_score, epoch)\n",
    "\n",
    "            if ES.early_stop(model, val_balanced_acc, epoch):\n",
    "                break\n",
    "    ES.save_best_model(best_model_path)\n",
    "    return model\n",
    "\n",
    "def test(\n",
    "    model:nn.Module,\n",
    "    criterion:nn.Module,\n",
    "    test_loader:DataLoader,\n",
    "    device:torch.device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"Test the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to test\n",
    "        criterion (nn.Module): Loss function to use\n",
    "        test_loader (DataLoader): Test data loader\n",
    "        device (torch.device, optional): Device on which calculations are executed. Defaults to torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float, float, float]: Tuple of predicted probabilities, \n",
    "            predicted labels, true labels and test loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    model_probs = []\n",
    "    y_preds = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for seq, label in test_loader:\n",
    "            seq, label = seq.to(device), label.to(device)\n",
    "            \n",
    "            #! Note Paul: I added this in order for the model not to give me any dimension issues!\n",
    "            seq=seq.squeeze(1)\n",
    "            output = model(seq)\n",
    "            \n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            model_probs.extend(F.softmax(output, dim=1)[:, 1].cpu().numpy())\n",
    "            y_true.extend(label.cpu().numpy())\n",
    "            y_preds.extend(F.softmax(output, dim=1).argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    \n",
    "    # calculate metrics\n",
    "    test_loss = total_loss / len(test_loader.dataset)\n",
    "    return model_probs, y_preds, y_true, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We based our CNN on this: https://medium.com/@chen-yu/building-a-customized-residual-cnn-with-pytorch-471810e894ed\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, classes_num: int, in_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial convolution layer\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=16, \n",
    "            #kernel_size=(3, 3), \n",
    "            kernel_size=3,\n",
    "            padding='same', \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        #Out features from the 4 blocks of residual layers: 128\n",
    "        \n",
    "        \n",
    "        # Flattening and final linear layer\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=16,\n",
    "            out_features=classes_num\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        #print(\"Shape after convolution and activation:\", x.shape)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        #print(\"Shape after flattening:\", x.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        #print(\"Shape after linear layer:\", x.shape)\n",
    "        return x\n",
    "    \n",
    "model=CNN(classes_num=2, in_channels=190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pauls\\miniconda3\\envs\\introtoml\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "model = model.to(DEVICE)\n",
    "init_parameters(model)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.005, weight_decay =0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, threshold=1e-06, verbose = 1)\n",
    "#scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
    "early_stopping = EarlyStopping(patience = 20, verbose = 1, mode = 'max')\n",
    "summary_writer = SummaryWriter(log_dir=\"logs/transformer_pe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Transformer: 100%|██████████| 100/100 [03:56<00:00,  2.36s/it, train_balanced_acc=0.905, train_loss=0.00609, val_balanced_acc=0.922, val_loss=0.00594]\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS=100\n",
    "\n",
    "model = train_and_validate(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler,\n",
    "    criterion = criterion,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    best_model_path = path+'\\cnn_best_model.pth',\n",
    "    device = DEVICE,\n",
    "    num_epochs = N_EPOCHS,\n",
    "    ES = early_stopping,\n",
    "    summary_writer = summary_writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\pauls\\\\Desktop\\\\Studium\\\\Machine Learning for Health Care\\\\Projekt 2\\\\project2_TS_inputcnn_best_model.pth'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path+'cnn_best_model.pth'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
