{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PTBDataset, test\n\u001b[0;32m     29\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "import math\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "from utils import PTBDataset, test\n",
    "from torch import Tensor\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'C:\\Users\\pauls\\Desktop\\Studium\\Machine Learning for Health Care\\Projekt 2\\project2_TS_input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(path+'\\ptbdb_train.csv', header=None)\n",
    "test_df = pd.read_csv(path+'\\ptbdb_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = train_df.iloc[:, :-1].to_numpy()\n",
    "y_train_full = train_df.iloc[:, -1].to_numpy()\n",
    "\n",
    "X_test = test_df.iloc[:, :-1].to_numpy()\n",
    "y_test = test_df.iloc[:, -1].to_numpy()\n",
    "\n",
    "X_train_full = np.c_[X_train_full, np.zeros((X_train_full.shape[0], 3))]\n",
    "X_test = np.c_[X_test, np.zeros((X_test.shape[0], 3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got this function from IML\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_loader_from_np(X, y = None, train = True, batch_size=32, shuffle=True, num_workers = 4):\n",
    "    \"\"\"\n",
    "    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n",
    "\n",
    "    input: X: numpy array, the features\n",
    "           y: numpy array, the labels\n",
    "    \n",
    "    output: loader: torch.data.util.DataLoader, the object containing the data\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        # Attention: If you get type errors you can modify the type of the\n",
    "        # labels here\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), \n",
    "                                torch.from_numpy(y).type(torch.long))\n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        pin_memory=True, num_workers=num_workers)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_loader_from_np(X_train, y_train, train = True, batch_size=32)\n",
    "val_loader = create_loader_from_np(X_val, y_val, train = True, shuffle = False, batch_size=32)\n",
    "test_loader = create_loader_from_np(X_test, y_test, train = False, shuffle = False, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies that I need because I can't import the utils file. Note: I made changes in train_one_epoch, validate_one_epoch and test_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script contains all the utility functions for the Transformer model.\n",
    "\n",
    "# =========================================================================== #\n",
    "#                              Packages and Presets                           #\n",
    "# =========================================================================== #\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.parameter import Parameter\n",
    "from tqdm import trange, tqdm\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, accuracy_score\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# =========================================================================== #\n",
    "#                 Data Loading and Preprocessing Utilities                    #\n",
    "# =========================================================================== #\n",
    "class PTB_Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(2)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        # Check if the input tensor is 3D, if so, add a batch dimension\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "# =========================================================================== #\n",
    "#                             General Utilities                               #                \n",
    "# =========================================================================== #\n",
    "# see https://vandurajan91.medium.com/random-seeds-and-reproducible-results-in-pytorch-211620301eba\n",
    "# for more information aboutreproducibility in pytorch\n",
    "def set_all_seeds(seed: int):\n",
    "    \"\"\"Set all possible seeds to ensure reproducibility and to avoid randomness\n",
    "    involved in GPU computations.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "#!!! This Early Stopping class is inspired by the following stackoverflow post:\n",
    "#!!! https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "#!!! and by this kaggle notebook:\n",
    "#!!! https://www.kaggle.com/code/megazotya/ecg-transformer/notebook\n",
    "class EarlyStopping:\n",
    "    def __init__(\n",
    "        self, \n",
    "        start: int = 50,\n",
    "        patience: int = 10,\n",
    "        epsilon: float = 1e-6,\n",
    "        verbose: bool = False,\n",
    "        mode: str = \"min\"\n",
    "    ):\n",
    "        self.start = start\n",
    "        self.counter = 0\n",
    "        self.patience = patience\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        self.mode = mode\n",
    "        \n",
    "        # initialize objects of which best value will be tracked\n",
    "        self.best_model = nn.Identity\n",
    "        self.best_epoch = 0\n",
    "        self.best_score = np.inf if mode == \"min\" else -np.inf\n",
    "        \n",
    "        \n",
    "    def early_stop(self, model: nn.Module, metric: float, current_epoch: int) -> bool:\n",
    "        \"\"\"Whether training should be stopped or not. If there was no improvement\n",
    "        in a long time, training should be stopped. Continuously saves the best\n",
    "        model.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current validation loss\n",
    "            model (nn.Module): Current model\n",
    "            current_epoch (int): Current epoch number\n",
    "\n",
    "        Returns:\n",
    "            bool: Whether training should be stopped or not\n",
    "        \"\"\"\n",
    "        # check whether improvement was large enough (if there was any at all)\n",
    "        if (\n",
    "            (metric < self.best_score + self.epsilon and self.mode == \"min\") or\n",
    "            (metric > self.best_score - self.epsilon and self.mode == \"max\")\n",
    "        ):\n",
    "            # reset number of epochs without improvement\n",
    "            self.counter = 0\n",
    "            # update best model and best score\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_score = metric\n",
    "            self.best_epoch = current_epoch\n",
    "            \n",
    "        elif current_epoch > self.start:\n",
    "            self.counter += 1  # stop training if no improvement in a long time\n",
    "            if self.counter >= self.patience:\n",
    "                if self.verbose:\n",
    "                    print(f\"Early stopping at epoch {current_epoch}. Best score was {self.best_score:.4f} in epoch {self.best_epoch}.\")\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def save_best_model(self, model_path:str) -> None:\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        torch.save(self.best_model, model_path)\n",
    "        \n",
    "    def get_best_model(self) -> nn.Module:\n",
    "        model = torch.load_state_dict(self.best_model)\n",
    "        return model\n",
    "\n",
    "\n",
    "# =========================================================================== #\n",
    "#                        Transformer Specific Utilities                       #\n",
    "# =========================================================================== #\n",
    "# Custom weight initialization:\n",
    "#!!! Copied from https://www.kaggle.com/code/megazotya/ecg-transformer/notebook\n",
    "def init_parameters(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "# =========================================================================== #\n",
    "#                    Model Training and Evaluation                            #\n",
    "# =========================================================================== #\n",
    "def train_one_epoch(\n",
    "    model:nn.Module,\n",
    "    optimizer:optim.Optimizer,\n",
    "    criterion:nn.Module,\n",
    "    train_loader:DataLoader,\n",
    "    device:torch.device\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"Train the model for one epoch\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to train\n",
    "        optimizer (optim.Optimizer): Optimizer to use\n",
    "        criterion (nn.Module): Loss function\n",
    "        train_loader (DataLoader): Train data loader\n",
    "        device (torch.device): Device on which calculations are executed\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float, float, float]: Tuple of train loss, accuracy, \n",
    "            balanced accuracy and f1 score\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    y_preds = []\n",
    "    y_true = []\n",
    "    for seq, label in train_loader:\n",
    "        seq, label = seq.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #! Note Paul: I added this in order for the model not to give me any dimension issues!\n",
    "        seq=seq.squeeze(1)\n",
    "        \n",
    "        output = model(seq)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        y_true.extend(label.cpu().numpy())\n",
    "        y_preds.extend(F.softmax(output, dim=1).argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    \n",
    "    # calculate metrics\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    train_acc = accuracy_score(y_true, y_preds)\n",
    "    train_balanced_acc = balanced_accuracy_score(y_true, y_preds)\n",
    "    train_f1_score = f1_score(y_true, y_preds)\n",
    "    return train_loss, train_acc, train_balanced_acc, train_f1_score\n",
    "\n",
    "def validate_one_epoch(\n",
    "    model:nn.Module,\n",
    "    criterion:nn.Module,\n",
    "    val_loader:DataLoader,\n",
    "    device:torch.device\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"Validate the model for one epoch\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to validate\n",
    "        criterion (nn.Module): Loss function to use\n",
    "        val_loader (DataLoader): Validation data loader\n",
    "        device (torch.device): Device on which calculations are executed\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float, float, float]: tuple of validation loss, accuracy,\n",
    "            balanced accuracy and f1 score\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    y_preds = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for seq, label in val_loader:\n",
    "            seq, label = seq.to(device), label.to(device)\n",
    "            \n",
    "            #! Note Paul: I added this in order for the model not to give me any dimension issues!\n",
    "            seq=seq.squeeze(1)\n",
    "            \n",
    "            output = model(seq)\n",
    "            \n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            y_true.extend(label.cpu().numpy())\n",
    "            y_preds.extend(F.softmax(output, dim=1).argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    \n",
    "    # calculate metrics\n",
    "    val_loss = total_loss / len(val_loader.dataset)\n",
    "    val_acc = accuracy_score(y_true, y_preds)\n",
    "    val_balanced_acc = balanced_accuracy_score(y_true, y_preds)\n",
    "    val_f1_score = f1_score(y_true, y_preds)\n",
    "    return val_loss, val_acc, val_balanced_acc, val_f1_score\n",
    "\n",
    "def train_and_validate(\n",
    "    model:nn.Module,\n",
    "    optimizer:optim.Optimizer,\n",
    "    scheduler:optim.lr_scheduler,\n",
    "    criterion:nn.Module,\n",
    "    train_loader:DataLoader,\n",
    "    val_loader:DataLoader,\n",
    "    best_model_path:str=\"weights/transformer_pe.pth\",\n",
    "    device:torch.device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    num_epochs:int=100,\n",
    "    ES:EarlyStopping=None,\n",
    "    summary_writer:SummaryWriter=None\n",
    "):\n",
    "    # make fancy progress bar\n",
    "    with trange(num_epochs) as t:\n",
    "        for epoch in t:\n",
    "            dct = {}\n",
    "            train_loss, train_acc, train_balanced_acc, train_f1_score = train_one_epoch(model, optimizer, criterion, train_loader, device)\n",
    "            val_loss, val_acc, val_balanced_acc, val_f1_score = validate_one_epoch(model, criterion, val_loader, device)\n",
    "            \n",
    "            # update progress bar\n",
    "            t.set_description(f\"Training Transformer\")\n",
    "            t.set_postfix(\n",
    "                train_loss=train_loss,\n",
    "                val_loss=val_loss,\n",
    "                train_balanced_acc=train_balanced_acc,\n",
    "                val_balanced_acc=val_balanced_acc\n",
    "            )\n",
    "            \n",
    "            \n",
    "            # reduce learning rate\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "            # log metrics to tensorboard\n",
    "            if summary_writer is not None:\n",
    "                summary_writer.add_scalar(\"Loss train\", train_loss, epoch)\n",
    "                summary_writer.add_scalar(\"Loss val\", val_loss, epoch)\n",
    "                summary_writer.add_scalar(\"Accuracy train\", train_acc, epoch)\n",
    "                summary_writer.add_scalar(\"Accuracy val\", val_acc, epoch)\n",
    "                summary_writer.add_scalar(\"Balanced accuracy train\", train_balanced_acc, epoch)\n",
    "                summary_writer.add_scalar(\"Balanced accuracy val\", val_balanced_acc, epoch)\n",
    "                summary_writer.add_scalar(\"F1 score train\", train_f1_score, epoch)\n",
    "                summary_writer.add_scalar(\"F1 score val\", val_f1_score, epoch)\n",
    "\n",
    "            if ES.early_stop(model, val_balanced_acc, epoch):\n",
    "                break\n",
    "    ES.save_best_model(best_model_path)\n",
    "    return model\n",
    "\n",
    "def test(\n",
    "    model:nn.Module,\n",
    "    criterion:nn.Module,\n",
    "    test_loader:DataLoader,\n",
    "    device:torch.device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"Test the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to test\n",
    "        criterion (nn.Module): Loss function to use\n",
    "        test_loader (DataLoader): Test data loader\n",
    "        device (torch.device, optional): Device on which calculations are executed. Defaults to torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float, float, float]: Tuple of predicted probabilities, \n",
    "            predicted labels, true labels and test loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    model_probs = []\n",
    "    y_preds = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for seq, label in test_loader:\n",
    "            seq, label = seq.to(device), label.to(device)\n",
    "            \n",
    "            #! Note Paul: I added this in order for the model not to give me any dimension issues!\n",
    "            seq=seq.squeeze(1)\n",
    "            output = model(seq)\n",
    "            \n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            model_probs.extend(F.softmax(output, dim=1)[:, 1].cpu().numpy())\n",
    "            y_true.extend(label.cpu().numpy())\n",
    "            y_preds.extend(F.softmax(output, dim=1).argmax(dim=1).cpu().numpy())\n",
    "\n",
    "    \n",
    "    # calculate metrics\n",
    "    test_loss = total_loss / len(test_loader.dataset)\n",
    "    return model_probs, y_preds, y_true, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We based our CNN on this: https://medium.com/@chen-yu/building-a-customized-residual-cnn-with-pytorch-471810e894ed\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, classes_num: int, in_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial convolution layer\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=16, \n",
    "            #kernel_size=(3, 3), \n",
    "            kernel_size=3,\n",
    "            padding='same', \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        #Out features from the 4 blocks of residual layers: 128\n",
    "        \n",
    "        \n",
    "        # Flattening and final linear layer\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=16,\n",
    "            out_features=classes_num\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        #print(\"Shape after convolution and activation:\", x.shape)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        #print(\"Shape after flattening:\", x.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        #print(\"Shape after linear layer:\", x.shape)\n",
    "        return x\n",
    "    \n",
    "model=CNN(classes_num=2, in_channels=190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "init_parameters(model)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.005, weight_decay =0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, threshold=1e-06, verbose = 1)\n",
    "#scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
    "early_stopping = EarlyStopping(patience = 20, verbose = 1, mode = 'max')\n",
    "summary_writer = SummaryWriter(log_dir=\"logs/transformer_pe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Transformer:   2%|▏         | 2/100 [00:03<02:47,  1.71s/it, train_balanced_acc=0.503, train_loss=0.0277, val_balanced_acc=0.49, val_loss=0.0279] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[165], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m N_EPOCHS\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcnn_best_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mES\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary_writer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msummary_writer\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[160], line 260\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[1;34m(model, optimizer, scheduler, criterion, train_loader, val_loader, best_model_path, device, num_epochs, ES, summary_writer)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m t:\n\u001b[0;32m    259\u001b[0m     dct \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 260\u001b[0m     train_loss, train_acc, train_balanced_acc, train_f1_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     val_loss, val_acc, val_balanced_acc, val_f1_score \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, criterion, val_loader, device)\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;66;03m# update progress bar\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[160], line 180\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, criterion, train_loader, device)\u001b[0m\n\u001b[0;32m    178\u001b[0m output \u001b[38;5;241m=\u001b[39m model(seq)\n\u001b[0;32m    179\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[1;32m--> 180\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    184\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\pauls\\miniconda3\\envs\\introtoml\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pauls\\miniconda3\\envs\\introtoml\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pauls\\miniconda3\\envs\\introtoml\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS=100\n",
    "\n",
    "model = train_and_validate(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler,\n",
    "    criterion = criterion,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    best_model_path = path+'cnn_best_model.pth',\n",
    "    device = DEVICE,\n",
    "    num_epochs = N_EPOCHS,\n",
    "    ES = early_stopping,\n",
    "    summary_writer = summary_writer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
